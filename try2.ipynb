{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download = True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = np.random.randn(input_dim, output_dim).astype(np.float64)\n",
    "        self.biases = np.zeros((1, output_dim)).astype(np.float64)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        return np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        # Compute gradients for weights and biases\n",
    "        self.d_weights = np.dot(self.inputs.T, d_output)  # Shape: (input_dim, output_dim)\n",
    "        self.d_biases = np.sum(d_output, axis=0, keepdims=True)  # Ensure shape: (1, output_dim)\n",
    "        return np.dot(d_output, self.weights.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, epsilon=1e-5, momentum=0.9):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.running_mean = 0\n",
    "        self.running_var = 0\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs  # Store inputs for use in the backward pass\n",
    "        self.mean = np.mean(inputs, axis=0)\n",
    "        self.var = np.var(inputs, axis=0)\n",
    "        self.normalized = (inputs - self.mean) / np.sqrt(self.var + self.epsilon)\n",
    "        return self.normalized\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        N, D = d_output.shape\n",
    "        d_normalized = d_output / np.sqrt(self.var + self.epsilon)\n",
    "        d_var = np.sum(d_output * (self.inputs - self.mean) * -0.5 * (self.var + self.epsilon)**(-1.5), axis=0)\n",
    "        d_mean = np.sum(d_output * -1 / np.sqrt(self.var + self.epsilon), axis=0) + d_var * np.mean(-2 * (self.inputs - self.mean), axis=0)\n",
    "        d_input = (d_normalized + d_var * 2 * (self.inputs - self.mean) / N + d_mean / N)\n",
    "        return d_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        return np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        return d_output * (self.inputs > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "\n",
    "    def forward(self, inputs, training=True):\n",
    "        if training:\n",
    "            self.mask = np.random.binomial(1, 1 - self.rate, size=inputs.shape) / (1 - self.rate)\n",
    "            return inputs * self.mask\n",
    "        return inputs\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        return d_output * self.mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "    def backward(self, predictions, labels):\n",
    "        m = labels.shape[0]\n",
    "        grad = predictions.copy()\n",
    "        grad[range(m), labels] -= 1\n",
    "        grad /= m\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, learning_rate=0.0001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = {}  # Moving averages of gradients\n",
    "        self.v = {}  # Moving averages of squared gradients\n",
    "        self.t = 0   # Time step\n",
    "\n",
    "    def update(self, param_name, param, grad):\n",
    "        # Initialize m and v if not already done\n",
    "        if param_name not in self.m:\n",
    "            self.m[param_name] = np.zeros_like(grad)\n",
    "            self.v[param_name] = np.zeros_like(grad)\n",
    "\n",
    "        # Increment time step\n",
    "        self.t += 1\n",
    "\n",
    "        # Update biased first moment estimate\n",
    "        self.m[param_name] = self.beta1 * self.m[param_name] + (1 - self.beta1) * grad\n",
    "\n",
    "        # Update biased second raw moment estimate\n",
    "        self.v[param_name] = self.beta2 * self.v[param_name] + (1 - self.beta2) * (grad ** 2)\n",
    "\n",
    "        # Correct bias in first and second moment estimates\n",
    "        m_hat = self.m[param_name] / (1 - self.beta1 ** self.t)\n",
    "        v_hat = self.v[param_name] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "        # Update parameters\n",
    "        update_value = self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        return update_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, architecture):\n",
    "        self.layers = []\n",
    "        self.build(architecture)\n",
    "\n",
    "    def build(self, architecture):\n",
    "        for layer in architecture:\n",
    "            if layer['type'] == 'Dense':\n",
    "                self.layers.append(Dense(layer['input_dim'], layer['output_dim']))\n",
    "            elif layer['type'] == 'BatchNormalization':\n",
    "                self.layers.append(BatchNormalization(layer['dim']))\n",
    "            elif layer['type'] == 'ReLU':\n",
    "                self.layers.append(ReLU())\n",
    "            elif layer['type'] == 'Dropout':\n",
    "                self.layers.append(Dropout(layer['rate']))\n",
    "            elif layer['type'] == 'Softmax':\n",
    "                self.layers.append(Softmax())\n",
    "        \n",
    "        self.optimizer = Adam()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "    \n",
    "    \n",
    "    def backward(self, d_output, labels=None):\n",
    "        for layer in reversed(self.layers):\n",
    "            if hasattr(layer, 'backward'):\n",
    "                if isinstance(layer, Softmax) and labels is not None:\n",
    "                    d_output = layer.backward(d_output, labels)  # Pass labels for softmax\n",
    "                else:\n",
    "                    d_output = layer.backward(d_output)  # No need for labels in other layers\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, Dense):\n",
    "                # Create unique names for weights and biases\n",
    "                weight_name = f\"layer_{idx}_weights\"\n",
    "                bias_name = f\"layer_{idx}_biases\"\n",
    "                \n",
    "                # Print statements to confirm shapes (for debugging)\n",
    "                # print(f\"Updating weights: {layer.weights.shape}, d_weights: {layer.d_weights.shape}\")\n",
    "                # print(f\"Updating biases: {layer.biases.shape}, d_biases: {layer.d_biases.shape}\")\n",
    "                \n",
    "                # Correctly pass parameters to the optimizer, including the gradient\n",
    "                layer.weights -= self.optimizer.update(weight_name, layer.weights, layer.d_weights)\n",
    "                layer.biases -= self.optimizer.update(bias_name, layer.biases, layer.d_biases)\n",
    "\n",
    "\n",
    "    def train(self, X, y, epochs, batch_size):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, len(X), batch_size):\n",
    "                X_batch = X[i:i + batch_size]\n",
    "                y_batch = y[i:i + batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                predictions = self.forward(X_batch)\n",
    "\n",
    "                # Compute loss (e.g., categorical cross-entropy)\n",
    "                loss = -np.mean(np.log(predictions[range(len(y_batch)), y_batch]))\n",
    "\n",
    "                # Backward pass\n",
    "                d_output = self.layers[-1].backward(predictions, y_batch)  # Use true labels here\n",
    "                self.backward(d_output, y_batch)\n",
    "\n",
    "                # Update weights\n",
    "                self.update()\n",
    "\n",
    "                if i % (batch_size * 10) == 0:  # Print every 10 batches\n",
    "                    print(f\"Epoch {epoch + 1}, Batch {i // batch_size}: Loss = {loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    def test(self, X_test, y_test):\n",
    "        predictions = self.forward(X_test)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)  # Get the index of the max log-probability\n",
    "        accuracy = np.mean(predicted_classes == y_test)  # Calculate accuracy\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0: Loss = 2.3025\n",
      "Epoch 2, Batch 0: Loss = 2.3007\n",
      "Epoch 3, Batch 0: Loss = 2.2985\n",
      "Epoch 4, Batch 0: Loss = 2.2980\n",
      "Epoch 5, Batch 0: Loss = 2.2965\n",
      "Epoch 6, Batch 0: Loss = 2.2951\n",
      "Epoch 7, Batch 0: Loss = 2.2885\n",
      "Epoch 8, Batch 0: Loss = 2.2860\n",
      "Epoch 9, Batch 0: Loss = 2.2839\n",
      "Epoch 10, Batch 0: Loss = 2.2830\n",
      "Epoch 11, Batch 0: Loss = 2.2776\n",
      "Epoch 12, Batch 0: Loss = 2.2747\n",
      "Epoch 13, Batch 0: Loss = 2.2730\n",
      "Epoch 14, Batch 0: Loss = 2.2649\n",
      "Epoch 15, Batch 0: Loss = 2.2641\n",
      "Epoch 16, Batch 0: Loss = 2.2629\n",
      "Epoch 17, Batch 0: Loss = 2.2584\n",
      "Epoch 18, Batch 0: Loss = 2.2567\n",
      "Epoch 19, Batch 0: Loss = 2.2510\n",
      "Epoch 20, Batch 0: Loss = 2.2478\n",
      "Epoch 21, Batch 0: Loss = 2.2401\n",
      "Epoch 22, Batch 0: Loss = 2.2373\n",
      "Epoch 23, Batch 0: Loss = 2.2301\n",
      "Epoch 24, Batch 0: Loss = 2.2291\n",
      "Epoch 25, Batch 0: Loss = 2.2189\n",
      "Epoch 26, Batch 0: Loss = 2.2197\n",
      "Epoch 27, Batch 0: Loss = 2.2124\n",
      "Epoch 28, Batch 0: Loss = 2.2062\n",
      "Epoch 29, Batch 0: Loss = 2.1990\n",
      "Epoch 30, Batch 0: Loss = 2.1978\n",
      "Epoch 31, Batch 0: Loss = 2.1846\n",
      "Epoch 32, Batch 0: Loss = 2.1837\n",
      "Epoch 33, Batch 0: Loss = 2.1823\n",
      "Epoch 34, Batch 0: Loss = 2.1679\n",
      "Epoch 35, Batch 0: Loss = 2.1622\n",
      "Epoch 36, Batch 0: Loss = 2.1531\n",
      "Epoch 37, Batch 0: Loss = 2.1429\n",
      "Epoch 38, Batch 0: Loss = 2.1461\n",
      "Epoch 39, Batch 0: Loss = 2.1389\n",
      "Epoch 40, Batch 0: Loss = 2.1243\n",
      "Epoch 41, Batch 0: Loss = 2.1186\n",
      "Epoch 42, Batch 0: Loss = 2.1029\n",
      "Epoch 43, Batch 0: Loss = 2.1005\n",
      "Epoch 44, Batch 0: Loss = 2.0862\n",
      "Epoch 45, Batch 0: Loss = 2.0811\n",
      "Epoch 46, Batch 0: Loss = 2.0765\n",
      "Epoch 47, Batch 0: Loss = 2.0489\n",
      "Epoch 48, Batch 0: Loss = 2.0457\n",
      "Epoch 49, Batch 0: Loss = 2.0174\n",
      "Epoch 50, Batch 0: Loss = 2.0256\n",
      "Test Accuracy: 43.75%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load your FMNIST dataset here\n",
    "    # X_train, y_train = ...\n",
    "\n",
    "    architecture = [\n",
    "        {'type': 'Dense', 'input_dim': 784, 'output_dim': 256},\n",
    "        {'type': 'BatchNormalization', 'dim': 256},\n",
    "        {'type': 'ReLU'},\n",
    "        {'type': 'Dropout', 'rate': 0.2},\n",
    "        {'type': 'Dense', 'input_dim': 256, 'output_dim': 128},\n",
    "        {'type': 'BatchNormalization', 'dim': 128},\n",
    "        {'type': 'ReLU'},\n",
    "        {'type': 'Dropout', 'rate': 0.2},\n",
    "        {'type': 'Dense', 'input_dim': 128, 'output_dim': 10},\n",
    "        {'type': 'Softmax'},\n",
    "    ]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "    # Prepare training data\n",
    "    X_train, y_train = next(iter(train_loader))\n",
    "    X_train = X_train.numpy().reshape(-1, 28*28)  # Flatten the images\n",
    "    y_train = y_train.numpy()\n",
    "\n",
    "\n",
    "    X_test, y_test = next(iter(test_loader))\n",
    "    X_test = X_test.numpy().reshape(-1, 28 * 28)  # Flatten the images\n",
    "    y_test = y_test.numpy()\n",
    "\n",
    "    X_train = X_train.astype(np.float32) / 255.0\n",
    "    X_test = X_test.astype(np.float32) / 255.0\n",
    "\n",
    "    \n",
    "    model = NeuralNetwork(architecture)\n",
    "    model.train(X_train, y_train, epochs=50, batch_size=32)\n",
    "    model.test(X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
