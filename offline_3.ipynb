{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        self.output = np.dot(X, self.weights) + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, d_output, learning_rate):\n",
    "        d_input = np.dot(d_output, self.weights.T)\n",
    "        d_weights = np.dot(self.input.T, d_output)\n",
    "        d_bias = np.sum(d_output, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * d_weights\n",
    "        self.bias -= learning_rate * d_bias\n",
    "        \n",
    "        return d_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, input_dim, epsilon=1e-8):\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = np.ones((1, input_dim))\n",
    "        self.beta = np.zeros((1, input_dim))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        self.variance = np.var(X, axis=0)\n",
    "        self.X_normalized = (X - self.mean) / np.sqrt(self.variance + self.epsilon)\n",
    "        self.output = self.gamma * self.X_normalized + self.beta\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, d_output, learning_rate):\n",
    "        N = d_output.shape[0]\n",
    "        \n",
    "        dX_normalized = d_output * self.gamma\n",
    "        d_variance = np.sum(dX_normalized * (self.input - self.mean) * -0.5 * np.power(self.variance + self.epsilon, -1.5), axis=0)\n",
    "        d_mean = np.sum(dX_normalized * -1 / np.sqrt(self.variance + self.epsilon), axis=0) + d_variance * np.mean(-2 * (self.input - self.mean), axis=0)\n",
    "        \n",
    "        d_input = (dX_normalized / np.sqrt(self.variance + self.epsilon)) + (d_variance * 2 * (self.input - self.mean) / N) + (d_mean / N)\n",
    "        \n",
    "        d_gamma = np.sum(d_output * self.X_normalized, axis=0)\n",
    "        d_beta = np.sum(d_output, axis=0)\n",
    "        \n",
    "        # Update gamma and beta\n",
    "        self.gamma -= learning_rate * d_gamma\n",
    "        self.beta -= learning_rate * d_beta\n",
    "        \n",
    "        return d_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def backward(self, d_output):\n",
    "        return d_output * (self.input > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "    \n",
    "    def forward(self, X, is_training=True):\n",
    "        if is_training:\n",
    "            self.mask = (np.random.rand(*X.shape) > self.rate)\n",
    "            return X * self.mask\n",
    "        return X\n",
    "    \n",
    "    def backward(self, d_output):\n",
    "        return d_output * self.mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy:\n",
    "    def forward(self, logits, labels):\n",
    "        # Apply softmax, then compute cross-entropy loss\n",
    "        self.logits = logits\n",
    "        self.labels = labels\n",
    "        \n",
    "        # Calculate softmax\n",
    "        exp_values = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.probs = probabilities\n",
    "        \n",
    "        # Calculate cross-entropy loss\n",
    "        batch_size = logits.shape[0]\n",
    "        correct_logprobs = -np.log(probabilities[range(batch_size), labels])\n",
    "        loss = np.sum(correct_logprobs) / batch_size\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, logits, labels):\n",
    "        # Compute gradient of the loss w.r.t logits\n",
    "        batch_size = logits.shape[0]\n",
    "        d_logits = self.probs\n",
    "        d_logits[range(batch_size), labels] -= 1\n",
    "        d_logits /= batch_size\n",
    "        \n",
    "        return d_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.params = params\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = [np.zeros_like(p) for p in self.params]\n",
    "        self.v = [np.zeros_like(p) for p in self.params]\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, grads):\n",
    "        self.t += 1\n",
    "        for i in range(len(self.params)):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grads[i]\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grads[i] ** 2)\n",
    "            \n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            self.params[i] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.loss_fn = SoftmaxCrossEntropy()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, d_output, learning_rate):\n",
    "        for layer in reversed(self.layers):\n",
    "            d_output = layer.backward(d_output, learning_rate)\n",
    "    \n",
    "    def train(self, X, Y, epochs, batch_size, learning_rate):\n",
    "            for epoch in range(epochs):\n",
    "                loss = 0\n",
    "                for i in range(0, len(X), batch_size):\n",
    "                    X_batch = X[i:i + batch_size]\n",
    "                    Y_batch = Y[i:i + batch_size]\n",
    "\n",
    "                    # Forward pass to get logits\n",
    "                    logits = self.forward(X_batch)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss += self.loss_fn.forward(logits, Y_batch)\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    d_output = self.loss_fn.backward(logits, Y_batch)\n",
    "                    self.backward(d_output, learning_rate)\n",
    "                \n",
    "                print(f\"Epoch {epoch + 1}, Loss: {loss / len(X)}\")\n",
    "\n",
    "    def save(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations to convert images to tensors and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to have mean 0.5 and std 0.5\n",
    "])\n",
    "\n",
    "# Download and load the FashionMNIST dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Convert data to numpy arrays\n",
    "def data_to_numpy(data_loader):\n",
    "    X, y = [], []\n",
    "    for images, labels in data_loader:\n",
    "        X.append(images.view(images.size(0), -1).numpy())  # Flatten 28x28 images\n",
    "        y.append(np.eye(10)[labels.numpy()])  # One-hot encode the labels\n",
    "    return np.vstack(X), np.vstack(y)\n",
    "\n",
    "train_data, train_labels = data_to_numpy(train_loader)\n",
    "test_data, test_labels = data_to_numpy(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# Initialize and train the model\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model \u001b[39m=\u001b[39m FeedForwardNN(layers)\n\u001b[0;32m---> 16\u001b[0m model\u001b[39m.\u001b[39;49mtrain(train_data, train_labels, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m)\n\u001b[1;32m     17\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mmodel.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 26\u001b[0m, in \u001b[0;36mFeedForwardNN.train\u001b[0;34m(self, X, Y, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     23\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(X_batch)\n\u001b[1;32m     25\u001b[0m \u001b[39m# Calculate loss\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_fn\u001b[39m.\u001b[39;49mforward(logits, Y_batch)\n\u001b[1;32m     28\u001b[0m \u001b[39m# Backward pass\u001b[39;00m\n\u001b[1;32m     29\u001b[0m d_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn\u001b[39m.\u001b[39mbackward(logits, Y_batch)\n",
      "Cell \u001b[0;32mIn[18], line 14\u001b[0m, in \u001b[0;36mSoftmaxCrossEntropy.forward\u001b[0;34m(self, logits, labels)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# Calculate cross-entropy loss\u001b[39;00m\n\u001b[1;32m     13\u001b[0m batch_size \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m correct_logprobs \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mlog(probabilities[\u001b[39mrange\u001b[39;49m(batch_size), labels])\n\u001b[1;32m     15\u001b[0m loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(correct_logprobs) \u001b[39m/\u001b[39m batch_size\n\u001b[1;32m     16\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "# Define the network architecture\n",
    "layers = [\n",
    "    DenseLayer(784, 256),\n",
    "    BatchNormalization(256),\n",
    "    ReLU(),\n",
    "    Dropout(0.2),\n",
    "    DenseLayer(256, 128),\n",
    "    BatchNormalization(128),\n",
    "    ReLU(),\n",
    "    Dropout(0.2),\n",
    "    DenseLayer(128, 10)\n",
    "]\n",
    "\n",
    "# Initialize and train the model\n",
    "model = FeedForwardNN(layers)\n",
    "model.train(train_data, train_labels, epochs=10, batch_size=64, learning_rate=0.001)\n",
    "model.save(\"model.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
