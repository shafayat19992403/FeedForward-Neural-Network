{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1002)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:23<00:00, 425012.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1002)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 94846.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1002)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:12<00:00, 128161.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1002)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 871478.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download = True)\n",
    "# test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download = True)\n",
    "train_dataset = datasets.MNIST(root='./data', train = True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train = False, transform=transform, download=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d(loss)/d(weights) = d(loss)/d(output) * d(output)/d(weights) = d_output * d(output)/d(weights)\n",
    "\n",
    "As output = weights * input\n",
    "d(output)/d(weights) = input\n",
    "\n",
    "So,\n",
    "d(loss)/d(weights) = d_output * input\n",
    "\n",
    "similarly for bias term. \n",
    "d(loss)/d(bias) = d_output * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialize weights with small random values and biases with zeros\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.biases = np.zeros((1, output_dim))\n",
    "\n",
    "        self.d_biases = None\n",
    "        self.d_weights = None\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Perform the forward pass\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, d_output):\n",
    "        # Compute the gradient for weights, biases, and inputs\n",
    "        self.d_weights = np.dot(self.inputs.T, d_output)\n",
    "        self.d_biases = np.sum(d_output, axis=0, keepdims=True)\n",
    "        d_inputs = np.dot(d_output, self.weights.T)\n",
    "        \n",
    "        \n",
    "        return d_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, num_features, momentum=0.9, epsilon=1e-5):\n",
    "        self.gamma = np.ones(num_features)  # Scale parameter\n",
    "        self.beta = np.zeros(num_features)   # Shift parameter\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Running averages for inference\n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.ones(num_features)\n",
    "\n",
    "        # To store intermediate values during forward pass\n",
    "        self.input = None\n",
    "        self.mean = None\n",
    "        self.var = None\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        if training:\n",
    "            self.mean = np.mean(x, axis=0)\n",
    "            self.var = np.var(x, axis=0)\n",
    "            self.input = x  # Save input for backward pass\n",
    "\n",
    "            # Normalize the input\n",
    "            x_normalized = (x - self.mean) / np.sqrt(self.var + self.epsilon)\n",
    "\n",
    "            # Update running averages\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
    "\n",
    "            # Scale and shift\n",
    "            return self.gamma * x_normalized + self.beta\n",
    "        else:\n",
    "            # During inference, use running averages\n",
    "            x_normalized = (x - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            return self.gamma * x_normalized + self.beta\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        N, D = self.input.shape  # Use stored input from forward pass\n",
    "\n",
    "        # Calculate gradients\n",
    "        dx_normalized = d_output * self.gamma\n",
    "\n",
    "        # Gradients for gamma and beta\n",
    "        dgamma = np.sum(d_output * (self.input - self.mean) / np.sqrt(self.var + self.epsilon), axis=0)\n",
    "        dbeta = np.sum(d_output, axis=0)\n",
    "\n",
    "        # Gradients for variance and mean\n",
    "        dvar = np.sum(dx_normalized * (self.input - self.mean) * -0.5 * np.power(self.var + self.epsilon, -1.5), axis=0)\n",
    "        dmean = np.sum(dx_normalized * -1 / np.sqrt(self.var + self.epsilon), axis=0) + dvar * np.mean(-2 * (self.input - self.mean), axis=0)\n",
    "\n",
    "        # Gradients for input\n",
    "        dx = (dx_normalized / np.sqrt(self.var + self.epsilon)) + (dvar * 2 * (self.input - self.mean) / N) + (dmean / N)\n",
    "\n",
    "        # Store gradients for the optimizer\n",
    "        self.d_gamma = dgamma\n",
    "        self.d_beta = dbeta\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, X):\n",
    "        self.cache = X\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dout):\n",
    "        # Gradient of ReLU: 1 where input > 0, else 0\n",
    "        dX = dout * (self.cache > 0)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            # Create a mask where each element is 0 with probability `dropout_rate`\n",
    "            self.mask = (np.random.rand(*X.shape) > self.dropout_rate) / (1 - self.dropout_rate)\n",
    "            # Apply the mask to the input\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            # During inference, do not apply dropout; use the original values\n",
    "            return X\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dout):\n",
    "        # During backpropagation, only propagate gradients through the neurons that were not dropped out\n",
    "        return dout * self.mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = {}\n",
    "        self.v = {}\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, w, dw, id):\n",
    "        if id not in self.m:\n",
    "            self.m[id] = np.zeros_like(w)\n",
    "            self.v[id] = np.zeros_like(w)\n",
    "\n",
    "        self.t += 1\n",
    "        self.m[id] = self.beta1 * self.m[id] + (1 - self.beta1) * dw\n",
    "        self.v[id] = self.beta2 * self.v[id] + (1 - self.beta2) * (dw ** 2)\n",
    "\n",
    "        m_hat = self.m[id] / (1 - self.beta1 ** self.t)\n",
    "        v_hat = self.v[id] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "        w -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, logits):\n",
    "        # Shift the logits by subtracting the max value for numerical stability\n",
    "        exp_values = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        # Normalize by dividing by the sum of exponentials along each row\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "        return probabilities\n",
    "\n",
    "    def backward(self, dL_dout):\n",
    "        # Gradient of the loss with respect to the input logits\n",
    "        dL_dz = self.output * (dL_dout - np.sum(dL_dout * self.output, axis=1, keepdims=True))\n",
    "        return dL_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    one_hot = np.zeros((labels.size, num_classes))\n",
    "    one_hot[np.arange(labels.size), labels] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(predictions, targets):\n",
    "    # Clip predictions to avoid log(0) which leads to NaN\n",
    "    targets_onehot = one_hot_encode(targets, 10)\n",
    "    # print(f'shape of pred:{predictions.shape} and shape of targets:{targets_onehot.shape}')\n",
    "    predictions = np.clip(predictions, 1e-15, 1 - 1e-15)\n",
    "    loss = -np.sum(targets_onehot * np.log(predictions)) / targets_onehot.shape[0]\n",
    "    return loss\n",
    "\n",
    "def derivative_cross_entropy(predictions, targets):\n",
    "\n",
    "    targets_onehot = one_hot_encode(targets, 10)\n",
    "    dL_dout = (predictions - targets_onehot) / targets_onehot.shape[0]\n",
    "    return dL_dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork:\n",
    "    def __init__(self, layers, optimizer, epochs=10, batch_size=64):\n",
    "        self.layers = layers\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train(self, train_loader):\n",
    "        for epoch in range(self.epochs):\n",
    "            loss = 0\n",
    "            for X_train, y_train in train_loader:\n",
    "                X_train = X_train.numpy().reshape(-1, 28*28)\n",
    "                y_train = y_train.numpy()\n",
    "                indices = np.arange(X_train.shape[0])\n",
    "                np.random.shuffle(indices)\n",
    "                X_train = X_train[indices]\n",
    "                y_train = y_train[indices]\n",
    "\n",
    "                for i in range(0, X_train.shape[0], self.batch_size):\n",
    "                    X_batch = X_train[i:i + self.batch_size]\n",
    "                    y_batch = y_train[i:i + self.batch_size]\n",
    "\n",
    "                    # Forward pass through the network\n",
    "                    input_data = X_batch\n",
    "                    for layer in self.layers:\n",
    "                        if isinstance(layer, Dropout):\n",
    "                            input_data = layer.forward(input_data, training=True)\n",
    "                        else:\n",
    "                            input_data = layer.forward(input_data)\n",
    "\n",
    "                    # Calculate loss\n",
    "                    loss = cross_entropy_loss(input_data, y_batch)\n",
    "                    dL_dout = derivative_cross_entropy(input_data, y_batch)\n",
    "\n",
    "                    # Backpropagation\n",
    "                    for layer in reversed(self.layers):\n",
    "                        dL_dout = layer.backward(dL_dout)\n",
    "\n",
    "                    # Update weights using Adam\n",
    "                    for layer in self.layers:\n",
    "                        if hasattr(layer, 'weights'):\n",
    "                            layer.weights = self.optimizer.update(layer.weights, layer.d_weights, id(layer.weights))\n",
    "                            layer.biases = self.optimizer.update(layer.biases, layer.d_biases, id(layer.biases))\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{self.epochs}, Loss: {loss / len(train_loader):.4f}')\n",
    "\n",
    "\n",
    "            \n",
    "    def test(self, test_loader):\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        for X_test, y_test in test_loader:\n",
    "            total_samples += X_test.shape[0]\n",
    "\n",
    "            # Forward pass through the network for the test set\n",
    "            X_test = X_test.view(X_test.size(0), -1).numpy()  # Flatten the images\n",
    "            y_test = y_test.numpy()\n",
    "\n",
    "            # Normalize input data\n",
    "            # input_data = X_test.astype(np.float32) / 255.0\n",
    "            input_data = X_test\n",
    "\n",
    "            for layer in self.layers:\n",
    "                input_data = layer.forward(input_data)\n",
    "\n",
    "            # Get predictions by taking the argmax of the output\n",
    "            predictions = np.argmax(input_data, axis=1)\n",
    "\n",
    "            # Compare predictions with true labels\n",
    "            correct_predictions += np.sum(predictions == y_test)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = correct_predictions / total_samples\n",
    "        print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.0003\n",
      "Epoch 2/5, Loss: 0.0003\n",
      "Epoch 3/5, Loss: 0.0001\n",
      "Epoch 4/5, Loss: 0.0002\n",
      "Epoch 5/5, Loss: 0.0001\n",
      "Test Accuracy: 96.65%\n"
     ]
    }
   ],
   "source": [
    "# layers = [\n",
    "#     DenseLayer(784, 128),\n",
    "#     ReLU(),\n",
    "#     BatchNormalization(128),\n",
    "#     Dropout(0.5),\n",
    "#     DenseLayer(128, 64),\n",
    "#     ReLU(),\n",
    "#     BatchNormalization(64),\n",
    "#     Dropout(0.5),\n",
    "#     DenseLayer(64, 32),\n",
    "#     ReLU(),\n",
    "#     BatchNormalization(32),\n",
    "#     Dropout(0.5),\n",
    "#     DenseLayer(32, 10),\n",
    "#     Softmax()\n",
    "# ]\n",
    "\n",
    "# Example: Adding more layers\n",
    "layers = [\n",
    "    DenseLayer(784, 256),\n",
    "    ReLU(),\n",
    "    BatchNormalization(256),\n",
    "    Dropout(0.2),\n",
    "    DenseLayer(256, 128),\n",
    "    ReLU(),\n",
    "    BatchNormalization(128),\n",
    "    Dropout(0.2),\n",
    "    DenseLayer(128, 64),\n",
    "    ReLU(),\n",
    "    BatchNormalization(64),\n",
    "    Dropout(0.2),\n",
    "    DenseLayer(64, 10),\n",
    "    Softmax()\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize Adam optimizer\n",
    "adam = Adam(learning_rate=0.0001)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "# Prepare training data\n",
    "# X_train, y_train = next(iter(train_loader))\n",
    "# X_train = X_train.numpy().reshape(-1, 28*28)  # Flatten the images\n",
    "# y_train = y_train.numpy()\n",
    "\n",
    "\n",
    "X_test, y_test = next(iter(test_loader))\n",
    "X_test = X_test.numpy().reshape(-1, 28 * 28)  # Flatten the images\n",
    "y_test = y_test.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create and train the network\n",
    "network = FeedForwardNetwork(layers=layers, optimizer=adam, epochs=5, batch_size=32)\n",
    "\n",
    "network.train(train_loader)\n",
    "    \n",
    "    \n",
    "\n",
    "network.test(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
