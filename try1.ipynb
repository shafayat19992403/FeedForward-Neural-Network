{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download = True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d(loss)/d(weights) = d(loss)/d(output) * d(output)/d(weights) = d_output * d(output)/d(weights)\n",
    "\n",
    "As output = weights * input\n",
    "d(output)/d(weights) = input\n",
    "\n",
    "So,\n",
    "d(loss)/d(weights) = d_output * input\n",
    "\n",
    "similarly for bias term. \n",
    "d(loss)/d(bias) = d_output * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialize weights with small random values and biases with zeros\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.biases = np.zeros((1, output_dim))\n",
    "\n",
    "        self.d_biases = None\n",
    "        self.d_weights = None\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Perform the forward pass\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, d_output):\n",
    "        # Compute the gradient for weights, biases, and inputs\n",
    "        self.d_weights = np.dot(self.inputs.T, d_output)\n",
    "        self.d_biases = np.sum(d_output, axis=0, keepdims=True)\n",
    "        d_inputs = np.dot(d_output, self.weights.T)\n",
    "        \n",
    "        # # Update weights and biases\n",
    "        # self.weights -= learning_rate * d_weights\n",
    "        # self.biases -= learning_rate * d_biases\n",
    "        \n",
    "        return d_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, num_features, momentum=0.9, epsilon=1e-5):\n",
    "        self.gamma = np.ones(num_features)  # Scale parameter\n",
    "        self.beta = np.zeros(num_features)   # Shift parameter\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Running averages for inference\n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.ones(num_features)\n",
    "\n",
    "        # To store intermediate values during forward pass\n",
    "        self.input = None\n",
    "        self.mean = None\n",
    "        self.var = None\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        if training:\n",
    "            self.mean = np.mean(x, axis=0)\n",
    "            self.var = np.var(x, axis=0)\n",
    "            self.input = x  # Save input for backward pass\n",
    "\n",
    "            # Normalize the input\n",
    "            x_normalized = (x - self.mean) / np.sqrt(self.var + self.epsilon)\n",
    "\n",
    "            # Update running averages\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
    "\n",
    "            # Scale and shift\n",
    "            return self.gamma * x_normalized + self.beta\n",
    "        else:\n",
    "            # During inference, use running averages\n",
    "            x_normalized = (x - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            return self.gamma * x_normalized + self.beta\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        N, D = self.input.shape  # Use stored input from forward pass\n",
    "\n",
    "        # Calculate gradients\n",
    "        dx_normalized = d_output * self.gamma\n",
    "\n",
    "        # Gradients for gamma and beta\n",
    "        dgamma = np.sum(d_output * (self.input - self.mean) / np.sqrt(self.var + self.epsilon), axis=0)\n",
    "        dbeta = np.sum(d_output, axis=0)\n",
    "\n",
    "        # Gradients for variance and mean\n",
    "        dvar = np.sum(dx_normalized * (self.input - self.mean) * -0.5 * np.power(self.var + self.epsilon, -1.5), axis=0)\n",
    "        dmean = np.sum(dx_normalized * -1 / np.sqrt(self.var + self.epsilon), axis=0) + dvar * np.mean(-2 * (self.input - self.mean), axis=0)\n",
    "\n",
    "        # Gradients for input\n",
    "        dx = (dx_normalized / np.sqrt(self.var + self.epsilon)) + (dvar * 2 * (self.input - self.mean) / N) + (dmean / N)\n",
    "\n",
    "        # Store gradients for the optimizer\n",
    "        self.d_gamma = dgamma\n",
    "        self.d_beta = dbeta\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, X):\n",
    "        self.cache = X\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dout):\n",
    "        # Gradient of ReLU: 1 where input > 0, else 0\n",
    "        dX = dout * (self.cache > 0)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            # Create a mask where each element is 0 with probability `dropout_rate`\n",
    "            self.mask = (np.random.rand(*X.shape) > self.dropout_rate) / (1 - self.dropout_rate)\n",
    "            # Apply the mask to the input\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            # During inference, do not apply dropout; use the original values\n",
    "            return X\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dout):\n",
    "        # During backpropagation, only propagate gradients through the neurons that were not dropped out\n",
    "        return dout * self.mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = {}\n",
    "        self.v = {}\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, w, dw, id):\n",
    "        if id not in self.m:\n",
    "            self.m[id] = np.zeros_like(w)\n",
    "            self.v[id] = np.zeros_like(w)\n",
    "\n",
    "        self.t += 1\n",
    "        self.m[id] = self.beta1 * self.m[id] + (1 - self.beta1) * dw\n",
    "        self.v[id] = self.beta2 * self.v[id] + (1 - self.beta2) * (dw ** 2)\n",
    "\n",
    "        m_hat = self.m[id] / (1 - self.beta1 ** self.t)\n",
    "        v_hat = self.v[id] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "        w -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, logits):\n",
    "        # Shift the logits by subtracting the max value for numerical stability\n",
    "        exp_values = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        # Normalize by dividing by the sum of exponentials along each row\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "        return probabilities\n",
    "\n",
    "    def backward(self, dL_dout):\n",
    "        # Gradient of the loss with respect to the input logits\n",
    "        dL_dz = self.output * (dL_dout - np.sum(dL_dout * self.output, axis=1, keepdims=True))\n",
    "        return dL_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(predictions, targets):\n",
    "        # One-hot encode targets\n",
    "        targets_one_hot = np.zeros_like(predictions)\n",
    "        targets_one_hot[np.arange(len(targets)), targets] = 1\n",
    "        return -np.mean(np.sum(targets_one_hot * np.log(predictions + 1e-12), axis=1))\n",
    "\n",
    "def derivative_cross_entropy(predictions, targets):\n",
    "        # One-hot encode targets\n",
    "        targets_one_hot = np.zeros_like(predictions)\n",
    "        targets_one_hot[np.arange(len(targets)), targets] = 1\n",
    "        return predictions - targets_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork:\n",
    "    def __init__(self, layers, optimizer, epochs=10, batch_size=64):\n",
    "        self.layers = layers\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        # self.learning_rate = learning_rate\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "            for epoch in range(self.epochs):\n",
    "                # Shuffle the training data\n",
    "                indices = np.arange(X_train.shape[0])\n",
    "                np.random.shuffle(indices)\n",
    "                X_train = X_train[indices]\n",
    "                y_train = y_train[indices]\n",
    "\n",
    "                for i in range(0, X_train.shape[0], self.batch_size):\n",
    "                    X_batch = X_train[i:i + self.batch_size]\n",
    "                    y_batch = y_train[i:i + self.batch_size]\n",
    "\n",
    "                    # Forward pass through the network\n",
    "                    input_data = X_batch\n",
    "                    for layer in self.layers:\n",
    "                        if isinstance(layer, Dropout):\n",
    "                            input_data = layer.forward(input_data, training=True)\n",
    "                        else:\n",
    "                            input_data = layer.forward(input_data)\n",
    "\n",
    "                    # Calculate loss and backward pass\n",
    "                    loss = cross_entropy_loss(input_data, y_batch)\n",
    "                    dL_dout = derivative_cross_entropy(input_data, y_batch)\n",
    "\n",
    "                    # Backpropagation\n",
    "                    for layer in reversed(self.layers):\n",
    "                        dL_dout = layer.backward(dL_dout)\n",
    "\n",
    "                    # Update weights using Adam\n",
    "                    for layer in self.layers:\n",
    "                        if hasattr(layer, 'weights'):\n",
    "                            layer.weights = self.optimizer.update(layer.weights, layer.d_weights, id(layer.weights))\n",
    "                            layer.biases = self.optimizer.update(layer.biases, layer.d_biases, id(layer.biases))\n",
    "\n",
    "                print(f'Epoch {epoch + 1}/{self.epochs}, Loss: {loss:.4f}')\n",
    "\n",
    "\n",
    "    def test(self, X_test, y_test):\n",
    "            correct_predictions = 0\n",
    "            total_samples = X_test.shape[0]\n",
    "\n",
    "            # Forward pass through the network for the test set\n",
    "            input_data = X_test\n",
    "            for layer in self.layers:\n",
    "                input_data = layer.forward(input_data)\n",
    "\n",
    "            # Get predictions by taking the argmax of the output\n",
    "            predictions = np.argmax(input_data, axis=1)\n",
    "\n",
    "            # Compare predictions with true labels\n",
    "            correct_predictions = np.sum(predictions == y_test)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            accuracy = correct_predictions / total_samples\n",
    "            print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 2.3042\n",
      "Epoch 2/1000, Loss: 2.2661\n",
      "Epoch 3/1000, Loss: 2.2723\n",
      "Epoch 4/1000, Loss: 2.2781\n",
      "Epoch 5/1000, Loss: 2.2529\n",
      "Epoch 6/1000, Loss: 2.2164\n",
      "Epoch 7/1000, Loss: 2.2073\n",
      "Epoch 8/1000, Loss: 2.1857\n",
      "Epoch 9/1000, Loss: 2.1900\n",
      "Epoch 10/1000, Loss: 2.1566\n",
      "Epoch 11/1000, Loss: 2.0972\n",
      "Epoch 12/1000, Loss: 2.0830\n",
      "Epoch 13/1000, Loss: 2.0775\n",
      "Epoch 14/1000, Loss: 2.0438\n",
      "Epoch 15/1000, Loss: 2.0060\n",
      "Epoch 16/1000, Loss: 1.9617\n",
      "Epoch 17/1000, Loss: 1.9361\n",
      "Epoch 18/1000, Loss: 1.8998\n",
      "Epoch 19/1000, Loss: 1.8649\n",
      "Epoch 20/1000, Loss: 1.8295\n",
      "Epoch 21/1000, Loss: 1.7551\n",
      "Epoch 22/1000, Loss: 1.7670\n",
      "Epoch 23/1000, Loss: 1.7340\n",
      "Epoch 24/1000, Loss: 1.7225\n",
      "Epoch 25/1000, Loss: 1.6671\n",
      "Epoch 26/1000, Loss: 1.6029\n",
      "Epoch 27/1000, Loss: 1.6142\n",
      "Epoch 28/1000, Loss: 1.5984\n",
      "Epoch 29/1000, Loss: 1.5715\n",
      "Epoch 30/1000, Loss: 1.5139\n",
      "Epoch 31/1000, Loss: 1.4705\n",
      "Epoch 32/1000, Loss: 1.3676\n",
      "Epoch 33/1000, Loss: 1.3826\n",
      "Epoch 34/1000, Loss: 1.3619\n",
      "Epoch 35/1000, Loss: 1.3194\n",
      "Epoch 36/1000, Loss: 1.3356\n",
      "Epoch 37/1000, Loss: 1.2988\n",
      "Epoch 38/1000, Loss: 1.2413\n",
      "Epoch 39/1000, Loss: 1.2724\n",
      "Epoch 40/1000, Loss: 1.2622\n",
      "Epoch 41/1000, Loss: 1.1695\n",
      "Epoch 42/1000, Loss: 1.1082\n",
      "Epoch 43/1000, Loss: 1.1147\n",
      "Epoch 44/1000, Loss: 1.1577\n",
      "Epoch 45/1000, Loss: 1.1423\n",
      "Epoch 46/1000, Loss: 1.0713\n",
      "Epoch 47/1000, Loss: 1.0602\n",
      "Epoch 48/1000, Loss: 1.0714\n",
      "Epoch 49/1000, Loss: 1.0308\n",
      "Epoch 50/1000, Loss: 1.0399\n",
      "Epoch 51/1000, Loss: 1.0127\n",
      "Epoch 52/1000, Loss: 0.9388\n",
      "Epoch 53/1000, Loss: 1.0043\n",
      "Epoch 54/1000, Loss: 0.9951\n",
      "Epoch 55/1000, Loss: 0.9866\n",
      "Epoch 56/1000, Loss: 0.9363\n",
      "Epoch 57/1000, Loss: 0.9742\n",
      "Epoch 58/1000, Loss: 0.8892\n",
      "Epoch 59/1000, Loss: 0.8906\n",
      "Epoch 60/1000, Loss: 0.8809\n",
      "Epoch 61/1000, Loss: 0.8195\n",
      "Epoch 62/1000, Loss: 0.7819\n",
      "Epoch 63/1000, Loss: 0.8372\n",
      "Epoch 64/1000, Loss: 0.8481\n",
      "Epoch 65/1000, Loss: 0.8937\n",
      "Epoch 66/1000, Loss: 0.8480\n",
      "Epoch 67/1000, Loss: 0.7686\n",
      "Epoch 68/1000, Loss: 0.8224\n",
      "Epoch 69/1000, Loss: 0.8206\n",
      "Epoch 70/1000, Loss: 0.7202\n",
      "Epoch 71/1000, Loss: 0.7796\n",
      "Epoch 72/1000, Loss: 0.8053\n",
      "Epoch 73/1000, Loss: 0.8319\n",
      "Epoch 74/1000, Loss: 0.7546\n",
      "Epoch 75/1000, Loss: 0.7284\n",
      "Epoch 76/1000, Loss: 0.6697\n",
      "Epoch 77/1000, Loss: 0.7713\n",
      "Epoch 78/1000, Loss: 0.7067\n",
      "Epoch 79/1000, Loss: 0.7317\n",
      "Epoch 80/1000, Loss: 0.6675\n",
      "Epoch 81/1000, Loss: 0.6098\n",
      "Epoch 82/1000, Loss: 0.5946\n",
      "Epoch 83/1000, Loss: 0.6387\n",
      "Epoch 84/1000, Loss: 0.6106\n",
      "Epoch 85/1000, Loss: 0.6092\n",
      "Epoch 86/1000, Loss: 0.6492\n",
      "Epoch 87/1000, Loss: 0.6784\n",
      "Epoch 88/1000, Loss: 0.6521\n",
      "Epoch 89/1000, Loss: 0.6480\n",
      "Epoch 90/1000, Loss: 0.5818\n",
      "Epoch 91/1000, Loss: 0.6079\n",
      "Epoch 92/1000, Loss: 0.6247\n",
      "Epoch 93/1000, Loss: 0.5717\n",
      "Epoch 94/1000, Loss: 0.6091\n",
      "Epoch 95/1000, Loss: 0.5409\n",
      "Epoch 96/1000, Loss: 0.4834\n",
      "Epoch 97/1000, Loss: 0.5623\n",
      "Epoch 98/1000, Loss: 0.5396\n",
      "Epoch 99/1000, Loss: 0.6510\n",
      "Epoch 100/1000, Loss: 0.5170\n",
      "Epoch 101/1000, Loss: 0.5115\n",
      "Epoch 102/1000, Loss: 0.5162\n",
      "Epoch 103/1000, Loss: 0.5992\n",
      "Epoch 104/1000, Loss: 0.4776\n",
      "Epoch 105/1000, Loss: 0.5455\n",
      "Epoch 106/1000, Loss: 0.5355\n",
      "Epoch 107/1000, Loss: 0.4644\n",
      "Epoch 108/1000, Loss: 0.5149\n",
      "Epoch 109/1000, Loss: 0.4030\n",
      "Epoch 110/1000, Loss: 0.4593\n",
      "Epoch 111/1000, Loss: 0.4261\n",
      "Epoch 112/1000, Loss: 0.4146\n",
      "Epoch 113/1000, Loss: 0.4330\n",
      "Epoch 114/1000, Loss: 0.4367\n",
      "Epoch 115/1000, Loss: 0.3858\n",
      "Epoch 116/1000, Loss: 0.4604\n",
      "Epoch 117/1000, Loss: 0.3926\n",
      "Epoch 118/1000, Loss: 0.3885\n",
      "Epoch 119/1000, Loss: 0.4197\n",
      "Epoch 120/1000, Loss: 0.3298\n",
      "Epoch 121/1000, Loss: 0.4026\n",
      "Epoch 122/1000, Loss: 0.4181\n",
      "Epoch 123/1000, Loss: 0.3638\n",
      "Epoch 124/1000, Loss: 0.3707\n",
      "Epoch 125/1000, Loss: 0.4198\n",
      "Epoch 126/1000, Loss: 0.3776\n",
      "Epoch 127/1000, Loss: 0.3297\n",
      "Epoch 128/1000, Loss: 0.3816\n",
      "Epoch 129/1000, Loss: 0.2943\n",
      "Epoch 130/1000, Loss: 0.3358\n",
      "Epoch 131/1000, Loss: 0.3723\n",
      "Epoch 132/1000, Loss: 0.3606\n",
      "Epoch 133/1000, Loss: 0.3029\n",
      "Epoch 134/1000, Loss: 0.3222\n",
      "Epoch 135/1000, Loss: 0.3360\n",
      "Epoch 136/1000, Loss: 0.3253\n",
      "Epoch 137/1000, Loss: 0.3542\n",
      "Epoch 138/1000, Loss: 0.2831\n",
      "Epoch 139/1000, Loss: 0.3044\n",
      "Epoch 140/1000, Loss: 0.3711\n",
      "Epoch 141/1000, Loss: 0.2893\n",
      "Epoch 142/1000, Loss: 0.3608\n",
      "Epoch 143/1000, Loss: 0.2839\n",
      "Epoch 144/1000, Loss: 0.2988\n",
      "Epoch 145/1000, Loss: 0.2612\n",
      "Epoch 146/1000, Loss: 0.2885\n",
      "Epoch 147/1000, Loss: 0.3312\n",
      "Epoch 148/1000, Loss: 0.2198\n",
      "Epoch 149/1000, Loss: 0.2565\n",
      "Epoch 150/1000, Loss: 0.2489\n",
      "Epoch 151/1000, Loss: 0.1964\n",
      "Epoch 152/1000, Loss: 0.2588\n",
      "Epoch 153/1000, Loss: 0.2373\n",
      "Epoch 154/1000, Loss: 0.2424\n",
      "Epoch 155/1000, Loss: 0.1429\n",
      "Epoch 156/1000, Loss: 0.2159\n",
      "Epoch 157/1000, Loss: 0.2503\n",
      "Epoch 158/1000, Loss: 0.2388\n",
      "Epoch 159/1000, Loss: 0.2381\n",
      "Epoch 160/1000, Loss: 0.2447\n",
      "Epoch 161/1000, Loss: 0.2927\n",
      "Epoch 162/1000, Loss: 0.2099\n",
      "Epoch 163/1000, Loss: 0.2711\n",
      "Epoch 164/1000, Loss: 0.2369\n",
      "Epoch 165/1000, Loss: 0.2578\n",
      "Epoch 166/1000, Loss: 0.1930\n",
      "Epoch 167/1000, Loss: 0.1967\n",
      "Epoch 168/1000, Loss: 0.3208\n",
      "Epoch 169/1000, Loss: 0.1932\n",
      "Epoch 170/1000, Loss: 0.2216\n",
      "Epoch 171/1000, Loss: 0.2463\n",
      "Epoch 172/1000, Loss: 0.2384\n",
      "Epoch 173/1000, Loss: 0.2316\n",
      "Epoch 174/1000, Loss: 0.2429\n",
      "Epoch 175/1000, Loss: 0.2215\n",
      "Epoch 176/1000, Loss: 0.2298\n",
      "Epoch 177/1000, Loss: 0.1843\n",
      "Epoch 178/1000, Loss: 0.1949\n",
      "Epoch 179/1000, Loss: 0.1896\n",
      "Epoch 180/1000, Loss: 0.2174\n",
      "Epoch 181/1000, Loss: 0.1631\n",
      "Epoch 182/1000, Loss: 0.1945\n",
      "Epoch 183/1000, Loss: 0.1957\n",
      "Epoch 184/1000, Loss: 0.1879\n",
      "Epoch 185/1000, Loss: 0.2046\n",
      "Epoch 186/1000, Loss: 0.2103\n",
      "Epoch 187/1000, Loss: 0.1607\n",
      "Epoch 188/1000, Loss: 0.1816\n",
      "Epoch 189/1000, Loss: 0.1580\n",
      "Epoch 190/1000, Loss: 0.2520\n",
      "Epoch 191/1000, Loss: 0.1829\n",
      "Epoch 192/1000, Loss: 0.1507\n",
      "Epoch 193/1000, Loss: 0.2422\n",
      "Epoch 194/1000, Loss: 0.1560\n",
      "Epoch 195/1000, Loss: 0.1927\n",
      "Epoch 196/1000, Loss: 0.1532\n",
      "Epoch 197/1000, Loss: 0.1885\n",
      "Epoch 198/1000, Loss: 0.2228\n",
      "Epoch 199/1000, Loss: 0.1561\n",
      "Epoch 200/1000, Loss: 0.1957\n",
      "Epoch 201/1000, Loss: 0.2232\n",
      "Epoch 202/1000, Loss: 0.1645\n",
      "Epoch 203/1000, Loss: 0.2386\n",
      "Epoch 204/1000, Loss: 0.2333\n",
      "Epoch 205/1000, Loss: 0.1682\n",
      "Epoch 206/1000, Loss: 0.1598\n",
      "Epoch 207/1000, Loss: 0.1278\n",
      "Epoch 208/1000, Loss: 0.1511\n",
      "Epoch 209/1000, Loss: 0.1637\n",
      "Epoch 210/1000, Loss: 0.1563\n",
      "Epoch 211/1000, Loss: 0.1883\n",
      "Epoch 212/1000, Loss: 0.1552\n",
      "Epoch 213/1000, Loss: 0.1248\n",
      "Epoch 214/1000, Loss: 0.1788\n",
      "Epoch 215/1000, Loss: 0.1497\n",
      "Epoch 216/1000, Loss: 0.1942\n",
      "Epoch 217/1000, Loss: 0.2340\n",
      "Epoch 218/1000, Loss: 0.1784\n",
      "Epoch 219/1000, Loss: 0.2000\n",
      "Epoch 220/1000, Loss: 0.1452\n",
      "Epoch 221/1000, Loss: 0.1882\n",
      "Epoch 222/1000, Loss: 0.1382\n",
      "Epoch 223/1000, Loss: 0.2538\n",
      "Epoch 224/1000, Loss: 0.1217\n",
      "Epoch 225/1000, Loss: 0.1375\n",
      "Epoch 226/1000, Loss: 0.1387\n",
      "Epoch 227/1000, Loss: 0.1588\n",
      "Epoch 228/1000, Loss: 0.1463\n",
      "Epoch 229/1000, Loss: 0.2062\n",
      "Epoch 230/1000, Loss: 0.1636\n",
      "Epoch 231/1000, Loss: 0.1647\n",
      "Epoch 232/1000, Loss: 0.1328\n",
      "Epoch 233/1000, Loss: 0.1746\n",
      "Epoch 234/1000, Loss: 0.1596\n",
      "Epoch 235/1000, Loss: 0.1864\n",
      "Epoch 236/1000, Loss: 0.1356\n",
      "Epoch 237/1000, Loss: 0.1546\n",
      "Epoch 238/1000, Loss: 0.1418\n",
      "Epoch 239/1000, Loss: 0.1156\n",
      "Epoch 240/1000, Loss: 0.1402\n",
      "Epoch 241/1000, Loss: 0.1157\n",
      "Epoch 242/1000, Loss: 0.1023\n",
      "Epoch 243/1000, Loss: 0.1587\n",
      "Epoch 244/1000, Loss: 0.2279\n",
      "Epoch 245/1000, Loss: 0.1907\n",
      "Epoch 246/1000, Loss: 0.1060\n",
      "Epoch 247/1000, Loss: 0.1687\n",
      "Epoch 248/1000, Loss: 0.1229\n",
      "Epoch 249/1000, Loss: 0.1385\n",
      "Epoch 250/1000, Loss: 0.1469\n",
      "Epoch 251/1000, Loss: 0.1795\n",
      "Epoch 252/1000, Loss: 0.1666\n",
      "Epoch 253/1000, Loss: 0.1505\n",
      "Epoch 254/1000, Loss: 0.1482\n",
      "Epoch 255/1000, Loss: 0.0976\n",
      "Epoch 256/1000, Loss: 0.1297\n",
      "Epoch 257/1000, Loss: 0.1338\n",
      "Epoch 258/1000, Loss: 0.1263\n",
      "Epoch 259/1000, Loss: 0.1365\n",
      "Epoch 260/1000, Loss: 0.1484\n",
      "Epoch 261/1000, Loss: 0.1245\n",
      "Epoch 262/1000, Loss: 0.1276\n",
      "Epoch 263/1000, Loss: 0.1370\n",
      "Epoch 264/1000, Loss: 0.1575\n",
      "Epoch 265/1000, Loss: 0.1227\n",
      "Epoch 266/1000, Loss: 0.1376\n",
      "Epoch 267/1000, Loss: 0.1194\n",
      "Epoch 268/1000, Loss: 0.1387\n",
      "Epoch 269/1000, Loss: 0.1278\n",
      "Epoch 270/1000, Loss: 0.1495\n",
      "Epoch 271/1000, Loss: 0.0894\n",
      "Epoch 272/1000, Loss: 0.0848\n",
      "Epoch 273/1000, Loss: 0.1411\n",
      "Epoch 274/1000, Loss: 0.1260\n",
      "Epoch 275/1000, Loss: 0.0895\n",
      "Epoch 276/1000, Loss: 0.0951\n",
      "Epoch 277/1000, Loss: 0.1139\n",
      "Epoch 278/1000, Loss: 0.1661\n",
      "Epoch 279/1000, Loss: 0.1529\n",
      "Epoch 280/1000, Loss: 0.1160\n",
      "Epoch 281/1000, Loss: 0.1135\n",
      "Epoch 282/1000, Loss: 0.0880\n",
      "Epoch 283/1000, Loss: 0.1146\n",
      "Epoch 284/1000, Loss: 0.1078\n",
      "Epoch 285/1000, Loss: 0.0904\n",
      "Epoch 286/1000, Loss: 0.1102\n",
      "Epoch 287/1000, Loss: 0.1115\n",
      "Epoch 288/1000, Loss: 0.1222\n",
      "Epoch 289/1000, Loss: 0.0807\n",
      "Epoch 290/1000, Loss: 0.1543\n",
      "Epoch 291/1000, Loss: 0.0691\n",
      "Epoch 292/1000, Loss: 0.0922\n",
      "Epoch 293/1000, Loss: 0.1070\n",
      "Epoch 294/1000, Loss: 0.0930\n",
      "Epoch 295/1000, Loss: 0.1643\n",
      "Epoch 296/1000, Loss: 0.0954\n",
      "Epoch 297/1000, Loss: 0.0880\n",
      "Epoch 298/1000, Loss: 0.0971\n",
      "Epoch 299/1000, Loss: 0.1470\n",
      "Epoch 300/1000, Loss: 0.1001\n",
      "Epoch 301/1000, Loss: 0.1213\n",
      "Epoch 302/1000, Loss: 0.0918\n",
      "Epoch 303/1000, Loss: 0.0924\n",
      "Epoch 304/1000, Loss: 0.0879\n",
      "Epoch 305/1000, Loss: 0.1116\n",
      "Epoch 306/1000, Loss: 0.1033\n",
      "Epoch 307/1000, Loss: 0.1165\n",
      "Epoch 308/1000, Loss: 0.1096\n",
      "Epoch 309/1000, Loss: 0.0863\n",
      "Epoch 310/1000, Loss: 0.1533\n",
      "Epoch 311/1000, Loss: 0.1138\n",
      "Epoch 312/1000, Loss: 0.1062\n",
      "Epoch 313/1000, Loss: 0.0825\n",
      "Epoch 314/1000, Loss: 0.1266\n",
      "Epoch 315/1000, Loss: 0.0835\n",
      "Epoch 316/1000, Loss: 0.1073\n",
      "Epoch 317/1000, Loss: 0.1171\n",
      "Epoch 318/1000, Loss: 0.0907\n",
      "Epoch 319/1000, Loss: 0.1324\n",
      "Epoch 320/1000, Loss: 0.0864\n",
      "Epoch 321/1000, Loss: 0.0969\n",
      "Epoch 322/1000, Loss: 0.0943\n",
      "Epoch 323/1000, Loss: 0.0709\n",
      "Epoch 324/1000, Loss: 0.0724\n",
      "Epoch 325/1000, Loss: 0.0783\n",
      "Epoch 326/1000, Loss: 0.0853\n",
      "Epoch 327/1000, Loss: 0.1025\n",
      "Epoch 328/1000, Loss: 0.0758\n",
      "Epoch 329/1000, Loss: 0.0934\n",
      "Epoch 330/1000, Loss: 0.0876\n",
      "Epoch 331/1000, Loss: 0.0659\n",
      "Epoch 332/1000, Loss: 0.1487\n",
      "Epoch 333/1000, Loss: 0.0815\n",
      "Epoch 334/1000, Loss: 0.1158\n",
      "Epoch 335/1000, Loss: 0.0751\n",
      "Epoch 336/1000, Loss: 0.1025\n",
      "Epoch 337/1000, Loss: 0.0686\n",
      "Epoch 338/1000, Loss: 0.1167\n",
      "Epoch 339/1000, Loss: 0.0812\n",
      "Epoch 340/1000, Loss: 0.0929\n",
      "Epoch 341/1000, Loss: 0.1259\n",
      "Epoch 342/1000, Loss: 0.0654\n",
      "Epoch 343/1000, Loss: 0.1302\n",
      "Epoch 344/1000, Loss: 0.0882\n",
      "Epoch 345/1000, Loss: 0.1166\n",
      "Epoch 346/1000, Loss: 0.0891\n",
      "Epoch 347/1000, Loss: 0.0975\n",
      "Epoch 348/1000, Loss: 0.0991\n",
      "Epoch 349/1000, Loss: 0.0926\n",
      "Epoch 350/1000, Loss: 0.1172\n",
      "Epoch 351/1000, Loss: 0.0928\n",
      "Epoch 352/1000, Loss: 0.0949\n",
      "Epoch 353/1000, Loss: 0.0820\n",
      "Epoch 354/1000, Loss: 0.1104\n",
      "Epoch 355/1000, Loss: 0.0864\n",
      "Epoch 356/1000, Loss: 0.0824\n",
      "Epoch 357/1000, Loss: 0.1123\n",
      "Epoch 358/1000, Loss: 0.1153\n",
      "Epoch 359/1000, Loss: 0.0661\n",
      "Epoch 360/1000, Loss: 0.1155\n",
      "Epoch 361/1000, Loss: 0.0849\n",
      "Epoch 362/1000, Loss: 0.1206\n",
      "Epoch 363/1000, Loss: 0.0719\n",
      "Epoch 364/1000, Loss: 0.0902\n",
      "Epoch 365/1000, Loss: 0.1281\n",
      "Epoch 366/1000, Loss: 0.0806\n",
      "Epoch 367/1000, Loss: 0.0497\n",
      "Epoch 368/1000, Loss: 0.0699\n",
      "Epoch 369/1000, Loss: 0.1044\n",
      "Epoch 370/1000, Loss: 0.0800\n",
      "Epoch 371/1000, Loss: 0.1009\n",
      "Epoch 372/1000, Loss: 0.0830\n",
      "Epoch 373/1000, Loss: 0.0896\n",
      "Epoch 374/1000, Loss: 0.0794\n",
      "Epoch 375/1000, Loss: 0.0664\n",
      "Epoch 376/1000, Loss: 0.1354\n",
      "Epoch 377/1000, Loss: 0.0866\n",
      "Epoch 378/1000, Loss: 0.0954\n",
      "Epoch 379/1000, Loss: 0.0863\n",
      "Epoch 380/1000, Loss: 0.0802\n",
      "Epoch 381/1000, Loss: 0.1364\n",
      "Epoch 382/1000, Loss: 0.0873\n",
      "Epoch 383/1000, Loss: 0.0950\n",
      "Epoch 384/1000, Loss: 0.1500\n",
      "Epoch 385/1000, Loss: 0.1089\n",
      "Epoch 386/1000, Loss: 0.0946\n",
      "Epoch 387/1000, Loss: 0.0670\n",
      "Epoch 388/1000, Loss: 0.0774\n",
      "Epoch 389/1000, Loss: 0.0713\n",
      "Epoch 390/1000, Loss: 0.0897\n",
      "Epoch 391/1000, Loss: 0.0832\n",
      "Epoch 392/1000, Loss: 0.1021\n",
      "Epoch 393/1000, Loss: 0.0552\n",
      "Epoch 394/1000, Loss: 0.0812\n",
      "Epoch 395/1000, Loss: 0.0720\n",
      "Epoch 396/1000, Loss: 0.0917\n",
      "Epoch 397/1000, Loss: 0.1014\n",
      "Epoch 398/1000, Loss: 0.0778\n",
      "Epoch 399/1000, Loss: 0.0545\n",
      "Epoch 400/1000, Loss: 0.0728\n",
      "Epoch 401/1000, Loss: 0.1086\n",
      "Epoch 402/1000, Loss: 0.1046\n",
      "Epoch 403/1000, Loss: 0.0891\n",
      "Epoch 404/1000, Loss: 0.0552\n",
      "Epoch 405/1000, Loss: 0.0744\n",
      "Epoch 406/1000, Loss: 0.0840\n",
      "Epoch 407/1000, Loss: 0.0683\n",
      "Epoch 408/1000, Loss: 0.0699\n",
      "Epoch 409/1000, Loss: 0.0723\n",
      "Epoch 410/1000, Loss: 0.0775\n",
      "Epoch 411/1000, Loss: 0.0903\n",
      "Epoch 412/1000, Loss: 0.0830\n",
      "Epoch 413/1000, Loss: 0.0703\n",
      "Epoch 414/1000, Loss: 0.0849\n",
      "Epoch 415/1000, Loss: 0.1278\n",
      "Epoch 416/1000, Loss: 0.1014\n",
      "Epoch 417/1000, Loss: 0.1110\n",
      "Epoch 418/1000, Loss: 0.0718\n",
      "Epoch 419/1000, Loss: 0.1104\n",
      "Epoch 420/1000, Loss: 0.1419\n",
      "Epoch 421/1000, Loss: 0.0691\n",
      "Epoch 422/1000, Loss: 0.0919\n",
      "Epoch 423/1000, Loss: 0.0878\n",
      "Epoch 424/1000, Loss: 0.0969\n",
      "Epoch 425/1000, Loss: 0.0970\n",
      "Epoch 426/1000, Loss: 0.0785\n",
      "Epoch 427/1000, Loss: 0.0723\n",
      "Epoch 428/1000, Loss: 0.0624\n",
      "Epoch 429/1000, Loss: 0.0816\n",
      "Epoch 430/1000, Loss: 0.0908\n",
      "Epoch 431/1000, Loss: 0.0498\n",
      "Epoch 432/1000, Loss: 0.0484\n",
      "Epoch 433/1000, Loss: 0.0576\n",
      "Epoch 434/1000, Loss: 0.0607\n",
      "Epoch 435/1000, Loss: 0.0519\n",
      "Epoch 436/1000, Loss: 0.0691\n",
      "Epoch 437/1000, Loss: 0.0706\n",
      "Epoch 438/1000, Loss: 0.0826\n",
      "Epoch 439/1000, Loss: 0.0600\n",
      "Epoch 440/1000, Loss: 0.0717\n",
      "Epoch 441/1000, Loss: 0.0617\n",
      "Epoch 442/1000, Loss: 0.0931\n",
      "Epoch 443/1000, Loss: 0.0650\n",
      "Epoch 444/1000, Loss: 0.0874\n",
      "Epoch 445/1000, Loss: 0.0678\n",
      "Epoch 446/1000, Loss: 0.0786\n",
      "Epoch 447/1000, Loss: 0.0798\n",
      "Epoch 448/1000, Loss: 0.0693\n",
      "Epoch 449/1000, Loss: 0.0948\n",
      "Epoch 450/1000, Loss: 0.0860\n",
      "Epoch 451/1000, Loss: 0.0659\n",
      "Epoch 452/1000, Loss: 0.0576\n",
      "Epoch 453/1000, Loss: 0.0754\n",
      "Epoch 454/1000, Loss: 0.0611\n",
      "Epoch 455/1000, Loss: 0.0833\n",
      "Epoch 456/1000, Loss: 0.0903\n",
      "Epoch 457/1000, Loss: 0.0653\n",
      "Epoch 458/1000, Loss: 0.0889\n",
      "Epoch 459/1000, Loss: 0.0979\n",
      "Epoch 460/1000, Loss: 0.0725\n",
      "Epoch 461/1000, Loss: 0.0946\n",
      "Epoch 462/1000, Loss: 0.0797\n",
      "Epoch 463/1000, Loss: 0.0483\n",
      "Epoch 464/1000, Loss: 0.0660\n",
      "Epoch 465/1000, Loss: 0.0857\n",
      "Epoch 466/1000, Loss: 0.0990\n",
      "Epoch 467/1000, Loss: 0.0850\n",
      "Epoch 468/1000, Loss: 0.0843\n",
      "Epoch 469/1000, Loss: 0.0531\n",
      "Epoch 470/1000, Loss: 0.0477\n",
      "Epoch 471/1000, Loss: 0.0939\n",
      "Epoch 472/1000, Loss: 0.0685\n",
      "Epoch 473/1000, Loss: 0.0508\n",
      "Epoch 474/1000, Loss: 0.1101\n",
      "Epoch 475/1000, Loss: 0.0796\n",
      "Epoch 476/1000, Loss: 0.0923\n",
      "Epoch 477/1000, Loss: 0.0706\n",
      "Epoch 478/1000, Loss: 0.0816\n",
      "Epoch 479/1000, Loss: 0.0587\n",
      "Epoch 480/1000, Loss: 0.0618\n",
      "Epoch 481/1000, Loss: 0.0957\n",
      "Epoch 482/1000, Loss: 0.0587\n",
      "Epoch 483/1000, Loss: 0.1258\n",
      "Epoch 484/1000, Loss: 0.0764\n",
      "Epoch 485/1000, Loss: 0.0803\n",
      "Epoch 486/1000, Loss: 0.0664\n",
      "Epoch 487/1000, Loss: 0.0826\n",
      "Epoch 488/1000, Loss: 0.0998\n",
      "Epoch 489/1000, Loss: 0.0509\n",
      "Epoch 490/1000, Loss: 0.0665\n",
      "Epoch 491/1000, Loss: 0.1020\n",
      "Epoch 492/1000, Loss: 0.0740\n",
      "Epoch 493/1000, Loss: 0.0439\n",
      "Epoch 494/1000, Loss: 0.0710\n",
      "Epoch 495/1000, Loss: 0.0602\n",
      "Epoch 496/1000, Loss: 0.1370\n",
      "Epoch 497/1000, Loss: 0.0735\n",
      "Epoch 498/1000, Loss: 0.0822\n",
      "Epoch 499/1000, Loss: 0.0980\n",
      "Epoch 500/1000, Loss: 0.1108\n",
      "Epoch 501/1000, Loss: 0.0632\n",
      "Epoch 502/1000, Loss: 0.1139\n",
      "Epoch 503/1000, Loss: 0.0505\n",
      "Epoch 504/1000, Loss: 0.0660\n",
      "Epoch 505/1000, Loss: 0.0697\n",
      "Epoch 506/1000, Loss: 0.0501\n",
      "Epoch 507/1000, Loss: 0.0564\n",
      "Epoch 508/1000, Loss: 0.0842\n",
      "Epoch 509/1000, Loss: 0.0655\n",
      "Epoch 510/1000, Loss: 0.0688\n",
      "Epoch 511/1000, Loss: 0.0713\n",
      "Epoch 512/1000, Loss: 0.0760\n",
      "Epoch 513/1000, Loss: 0.0579\n",
      "Epoch 514/1000, Loss: 0.0821\n",
      "Epoch 515/1000, Loss: 0.0780\n",
      "Epoch 516/1000, Loss: 0.0732\n",
      "Epoch 517/1000, Loss: 0.0402\n",
      "Epoch 518/1000, Loss: 0.0934\n",
      "Epoch 519/1000, Loss: 0.0575\n",
      "Epoch 520/1000, Loss: 0.0766\n",
      "Epoch 521/1000, Loss: 0.0805\n",
      "Epoch 522/1000, Loss: 0.0463\n",
      "Epoch 523/1000, Loss: 0.0709\n",
      "Epoch 524/1000, Loss: 0.1208\n",
      "Epoch 525/1000, Loss: 0.0762\n",
      "Epoch 526/1000, Loss: 0.0577\n",
      "Epoch 527/1000, Loss: 0.0653\n",
      "Epoch 528/1000, Loss: 0.0471\n",
      "Epoch 529/1000, Loss: 0.0770\n",
      "Epoch 530/1000, Loss: 0.0562\n",
      "Epoch 531/1000, Loss: 0.0547\n",
      "Epoch 532/1000, Loss: 0.0441\n",
      "Epoch 533/1000, Loss: 0.0699\n",
      "Epoch 534/1000, Loss: 0.0604\n",
      "Epoch 535/1000, Loss: 0.0630\n",
      "Epoch 536/1000, Loss: 0.0592\n",
      "Epoch 537/1000, Loss: 0.1063\n",
      "Epoch 538/1000, Loss: 0.0696\n",
      "Epoch 539/1000, Loss: 0.0516\n",
      "Epoch 540/1000, Loss: 0.0975\n",
      "Epoch 541/1000, Loss: 0.0717\n",
      "Epoch 542/1000, Loss: 0.1178\n",
      "Epoch 543/1000, Loss: 0.0510\n",
      "Epoch 544/1000, Loss: 0.0767\n",
      "Epoch 545/1000, Loss: 0.0472\n",
      "Epoch 546/1000, Loss: 0.0559\n",
      "Epoch 547/1000, Loss: 0.0700\n",
      "Epoch 548/1000, Loss: 0.0821\n",
      "Epoch 549/1000, Loss: 0.0541\n",
      "Epoch 550/1000, Loss: 0.0916\n",
      "Epoch 551/1000, Loss: 0.0713\n",
      "Epoch 552/1000, Loss: 0.0587\n",
      "Epoch 553/1000, Loss: 0.0526\n",
      "Epoch 554/1000, Loss: 0.0767\n",
      "Epoch 555/1000, Loss: 0.0409\n",
      "Epoch 556/1000, Loss: 0.0692\n",
      "Epoch 557/1000, Loss: 0.0560\n",
      "Epoch 558/1000, Loss: 0.0769\n",
      "Epoch 559/1000, Loss: 0.0636\n",
      "Epoch 560/1000, Loss: 0.0694\n",
      "Epoch 561/1000, Loss: 0.0796\n",
      "Epoch 562/1000, Loss: 0.0563\n",
      "Epoch 563/1000, Loss: 0.0616\n",
      "Epoch 564/1000, Loss: 0.0626\n",
      "Epoch 565/1000, Loss: 0.0754\n",
      "Epoch 566/1000, Loss: 0.0528\n",
      "Epoch 567/1000, Loss: 0.0957\n",
      "Epoch 568/1000, Loss: 0.0709\n",
      "Epoch 569/1000, Loss: 0.0328\n",
      "Epoch 570/1000, Loss: 0.0597\n",
      "Epoch 571/1000, Loss: 0.0523\n",
      "Epoch 572/1000, Loss: 0.0585\n",
      "Epoch 573/1000, Loss: 0.0658\n",
      "Epoch 574/1000, Loss: 0.0385\n",
      "Epoch 575/1000, Loss: 0.0526\n",
      "Epoch 576/1000, Loss: 0.0645\n",
      "Epoch 577/1000, Loss: 0.0523\n",
      "Epoch 578/1000, Loss: 0.0802\n",
      "Epoch 579/1000, Loss: 0.0606\n",
      "Epoch 580/1000, Loss: 0.0676\n",
      "Epoch 581/1000, Loss: 0.0744\n",
      "Epoch 582/1000, Loss: 0.0516\n",
      "Epoch 583/1000, Loss: 0.0549\n",
      "Epoch 584/1000, Loss: 0.0547\n",
      "Epoch 585/1000, Loss: 0.0544\n",
      "Epoch 586/1000, Loss: 0.0290\n",
      "Epoch 587/1000, Loss: 0.0495\n",
      "Epoch 588/1000, Loss: 0.0630\n",
      "Epoch 589/1000, Loss: 0.0674\n",
      "Epoch 590/1000, Loss: 0.0522\n",
      "Epoch 591/1000, Loss: 0.0699\n",
      "Epoch 592/1000, Loss: 0.0714\n",
      "Epoch 593/1000, Loss: 0.0645\n",
      "Epoch 594/1000, Loss: 0.0551\n",
      "Epoch 595/1000, Loss: 0.0517\n",
      "Epoch 596/1000, Loss: 0.0781\n",
      "Epoch 597/1000, Loss: 0.0518\n",
      "Epoch 598/1000, Loss: 0.0841\n",
      "Epoch 599/1000, Loss: 0.0737\n",
      "Epoch 600/1000, Loss: 0.0612\n",
      "Epoch 601/1000, Loss: 0.0565\n",
      "Epoch 602/1000, Loss: 0.0643\n",
      "Epoch 603/1000, Loss: 0.0655\n",
      "Epoch 604/1000, Loss: 0.0607\n",
      "Epoch 605/1000, Loss: 0.0534\n",
      "Epoch 606/1000, Loss: 0.0576\n",
      "Epoch 607/1000, Loss: 0.0671\n",
      "Epoch 608/1000, Loss: 0.0820\n",
      "Epoch 609/1000, Loss: 0.0397\n",
      "Epoch 610/1000, Loss: 0.0532\n",
      "Epoch 611/1000, Loss: 0.0584\n",
      "Epoch 612/1000, Loss: 0.0770\n",
      "Epoch 613/1000, Loss: 0.0448\n",
      "Epoch 614/1000, Loss: 0.0493\n",
      "Epoch 615/1000, Loss: 0.0697\n",
      "Epoch 616/1000, Loss: 0.0658\n",
      "Epoch 617/1000, Loss: 0.0564\n",
      "Epoch 618/1000, Loss: 0.0809\n",
      "Epoch 619/1000, Loss: 0.0665\n",
      "Epoch 620/1000, Loss: 0.0571\n",
      "Epoch 621/1000, Loss: 0.1086\n",
      "Epoch 622/1000, Loss: 0.0710\n",
      "Epoch 623/1000, Loss: 0.0669\n",
      "Epoch 624/1000, Loss: 0.0408\n",
      "Epoch 625/1000, Loss: 0.0536\n",
      "Epoch 626/1000, Loss: 0.0443\n",
      "Epoch 627/1000, Loss: 0.0668\n",
      "Epoch 628/1000, Loss: 0.0575\n",
      "Epoch 629/1000, Loss: 0.0563\n",
      "Epoch 630/1000, Loss: 0.0486\n",
      "Epoch 631/1000, Loss: 0.0890\n",
      "Epoch 632/1000, Loss: 0.0513\n",
      "Epoch 633/1000, Loss: 0.0464\n",
      "Epoch 634/1000, Loss: 0.0610\n",
      "Epoch 635/1000, Loss: 0.0609\n",
      "Epoch 636/1000, Loss: 0.0330\n",
      "Epoch 637/1000, Loss: 0.0543\n",
      "Epoch 638/1000, Loss: 0.0600\n",
      "Epoch 639/1000, Loss: 0.0763\n",
      "Epoch 640/1000, Loss: 0.0884\n",
      "Epoch 641/1000, Loss: 0.0582\n",
      "Epoch 642/1000, Loss: 0.0578\n",
      "Epoch 643/1000, Loss: 0.0884\n",
      "Epoch 644/1000, Loss: 0.0768\n",
      "Epoch 645/1000, Loss: 0.0773\n",
      "Epoch 646/1000, Loss: 0.0435\n",
      "Epoch 647/1000, Loss: 0.0573\n",
      "Epoch 648/1000, Loss: 0.0463\n",
      "Epoch 649/1000, Loss: 0.0547\n",
      "Epoch 650/1000, Loss: 0.0693\n",
      "Epoch 651/1000, Loss: 0.0350\n",
      "Epoch 652/1000, Loss: 0.0490\n",
      "Epoch 653/1000, Loss: 0.0525\n",
      "Epoch 654/1000, Loss: 0.0365\n",
      "Epoch 655/1000, Loss: 0.0352\n",
      "Epoch 656/1000, Loss: 0.0637\n",
      "Epoch 657/1000, Loss: 0.0289\n",
      "Epoch 658/1000, Loss: 0.0493\n",
      "Epoch 659/1000, Loss: 0.0516\n",
      "Epoch 660/1000, Loss: 0.0304\n",
      "Epoch 661/1000, Loss: 0.0662\n",
      "Epoch 662/1000, Loss: 0.0398\n",
      "Epoch 663/1000, Loss: 0.0555\n",
      "Epoch 664/1000, Loss: 0.0320\n",
      "Epoch 665/1000, Loss: 0.0375\n",
      "Epoch 666/1000, Loss: 0.0324\n",
      "Epoch 667/1000, Loss: 0.0746\n",
      "Epoch 668/1000, Loss: 0.0379\n",
      "Epoch 669/1000, Loss: 0.0562\n",
      "Epoch 670/1000, Loss: 0.0665\n",
      "Epoch 671/1000, Loss: 0.0687\n",
      "Epoch 672/1000, Loss: 0.0901\n",
      "Epoch 673/1000, Loss: 0.0648\n",
      "Epoch 674/1000, Loss: 0.0383\n",
      "Epoch 675/1000, Loss: 0.0551\n",
      "Epoch 676/1000, Loss: 0.0762\n",
      "Epoch 677/1000, Loss: 0.0456\n",
      "Epoch 678/1000, Loss: 0.0477\n",
      "Epoch 679/1000, Loss: 0.0660\n",
      "Epoch 680/1000, Loss: 0.0465\n",
      "Epoch 681/1000, Loss: 0.0559\n",
      "Epoch 682/1000, Loss: 0.0240\n",
      "Epoch 683/1000, Loss: 0.0769\n",
      "Epoch 684/1000, Loss: 0.0604\n",
      "Epoch 685/1000, Loss: 0.0399\n",
      "Epoch 686/1000, Loss: 0.0490\n",
      "Epoch 687/1000, Loss: 0.0674\n",
      "Epoch 688/1000, Loss: 0.0688\n",
      "Epoch 689/1000, Loss: 0.0488\n",
      "Epoch 690/1000, Loss: 0.0525\n",
      "Epoch 691/1000, Loss: 0.0460\n",
      "Epoch 692/1000, Loss: 0.0414\n",
      "Epoch 693/1000, Loss: 0.0396\n",
      "Epoch 694/1000, Loss: 0.0535\n",
      "Epoch 695/1000, Loss: 0.0486\n",
      "Epoch 696/1000, Loss: 0.0381\n",
      "Epoch 697/1000, Loss: 0.0384\n",
      "Epoch 698/1000, Loss: 0.0688\n",
      "Epoch 699/1000, Loss: 0.0699\n",
      "Epoch 700/1000, Loss: 0.0613\n",
      "Epoch 701/1000, Loss: 0.0689\n",
      "Epoch 702/1000, Loss: 0.0456\n",
      "Epoch 703/1000, Loss: 0.0460\n",
      "Epoch 704/1000, Loss: 0.0394\n",
      "Epoch 705/1000, Loss: 0.0621\n",
      "Epoch 706/1000, Loss: 0.0725\n",
      "Epoch 707/1000, Loss: 0.0822\n",
      "Epoch 708/1000, Loss: 0.0491\n",
      "Epoch 709/1000, Loss: 0.0490\n",
      "Epoch 710/1000, Loss: 0.0380\n",
      "Epoch 711/1000, Loss: 0.0463\n",
      "Epoch 712/1000, Loss: 0.0348\n",
      "Epoch 713/1000, Loss: 0.0708\n",
      "Epoch 714/1000, Loss: 0.0616\n",
      "Epoch 715/1000, Loss: 0.0392\n",
      "Epoch 716/1000, Loss: 0.0653\n",
      "Epoch 717/1000, Loss: 0.0516\n",
      "Epoch 718/1000, Loss: 0.0570\n",
      "Epoch 719/1000, Loss: 0.0508\n",
      "Epoch 720/1000, Loss: 0.0442\n",
      "Epoch 721/1000, Loss: 0.0751\n",
      "Epoch 722/1000, Loss: 0.0647\n",
      "Epoch 723/1000, Loss: 0.0541\n",
      "Epoch 724/1000, Loss: 0.0427\n",
      "Epoch 725/1000, Loss: 0.0595\n",
      "Epoch 726/1000, Loss: 0.0592\n",
      "Epoch 727/1000, Loss: 0.0642\n",
      "Epoch 728/1000, Loss: 0.0608\n",
      "Epoch 729/1000, Loss: 0.0698\n",
      "Epoch 730/1000, Loss: 0.0583\n",
      "Epoch 731/1000, Loss: 0.0607\n",
      "Epoch 732/1000, Loss: 0.0471\n",
      "Epoch 733/1000, Loss: 0.0426\n",
      "Epoch 734/1000, Loss: 0.0471\n",
      "Epoch 735/1000, Loss: 0.0370\n",
      "Epoch 736/1000, Loss: 0.0528\n",
      "Epoch 737/1000, Loss: 0.0551\n",
      "Epoch 738/1000, Loss: 0.0719\n",
      "Epoch 739/1000, Loss: 0.0408\n",
      "Epoch 740/1000, Loss: 0.0681\n",
      "Epoch 741/1000, Loss: 0.0521\n",
      "Epoch 742/1000, Loss: 0.0485\n",
      "Epoch 743/1000, Loss: 0.0652\n",
      "Epoch 744/1000, Loss: 0.0315\n",
      "Epoch 745/1000, Loss: 0.0428\n",
      "Epoch 746/1000, Loss: 0.0366\n",
      "Epoch 747/1000, Loss: 0.0351\n",
      "Epoch 748/1000, Loss: 0.0733\n",
      "Epoch 749/1000, Loss: 0.0591\n",
      "Epoch 750/1000, Loss: 0.0471\n",
      "Epoch 751/1000, Loss: 0.0247\n",
      "Epoch 752/1000, Loss: 0.0358\n",
      "Epoch 753/1000, Loss: 0.0383\n",
      "Epoch 754/1000, Loss: 0.0377\n",
      "Epoch 755/1000, Loss: 0.0312\n",
      "Epoch 756/1000, Loss: 0.0279\n",
      "Epoch 757/1000, Loss: 0.0543\n",
      "Epoch 758/1000, Loss: 0.0330\n",
      "Epoch 759/1000, Loss: 0.0422\n",
      "Epoch 760/1000, Loss: 0.0508\n",
      "Epoch 761/1000, Loss: 0.0315\n",
      "Epoch 762/1000, Loss: 0.0443\n",
      "Epoch 763/1000, Loss: 0.0530\n",
      "Epoch 764/1000, Loss: 0.0387\n",
      "Epoch 765/1000, Loss: 0.0345\n",
      "Epoch 766/1000, Loss: 0.0398\n",
      "Epoch 767/1000, Loss: 0.0487\n",
      "Epoch 768/1000, Loss: 0.0441\n",
      "Epoch 769/1000, Loss: 0.0417\n",
      "Epoch 770/1000, Loss: 0.0649\n",
      "Epoch 771/1000, Loss: 0.0621\n",
      "Epoch 772/1000, Loss: 0.0306\n",
      "Epoch 773/1000, Loss: 0.0451\n",
      "Epoch 774/1000, Loss: 0.0369\n",
      "Epoch 775/1000, Loss: 0.0445\n",
      "Epoch 776/1000, Loss: 0.0227\n",
      "Epoch 777/1000, Loss: 0.0392\n",
      "Epoch 778/1000, Loss: 0.0386\n",
      "Epoch 779/1000, Loss: 0.0392\n",
      "Epoch 780/1000, Loss: 0.0359\n",
      "Epoch 781/1000, Loss: 0.0653\n",
      "Epoch 782/1000, Loss: 0.0433\n",
      "Epoch 783/1000, Loss: 0.0300\n",
      "Epoch 784/1000, Loss: 0.0630\n",
      "Epoch 785/1000, Loss: 0.0451\n",
      "Epoch 786/1000, Loss: 0.0379\n",
      "Epoch 787/1000, Loss: 0.0351\n",
      "Epoch 788/1000, Loss: 0.0352\n",
      "Epoch 789/1000, Loss: 0.0353\n",
      "Epoch 790/1000, Loss: 0.0294\n",
      "Epoch 791/1000, Loss: 0.0366\n",
      "Epoch 792/1000, Loss: 0.0620\n",
      "Epoch 793/1000, Loss: 0.0522\n",
      "Epoch 794/1000, Loss: 0.0559\n",
      "Epoch 795/1000, Loss: 0.0458\n",
      "Epoch 796/1000, Loss: 0.0305\n",
      "Epoch 797/1000, Loss: 0.0590\n",
      "Epoch 798/1000, Loss: 0.0701\n",
      "Epoch 799/1000, Loss: 0.0357\n",
      "Epoch 800/1000, Loss: 0.0503\n",
      "Epoch 801/1000, Loss: 0.0658\n",
      "Epoch 802/1000, Loss: 0.0465\n",
      "Epoch 803/1000, Loss: 0.0428\n",
      "Epoch 804/1000, Loss: 0.0477\n",
      "Epoch 805/1000, Loss: 0.0451\n",
      "Epoch 806/1000, Loss: 0.0465\n",
      "Epoch 807/1000, Loss: 0.0227\n",
      "Epoch 808/1000, Loss: 0.0542\n",
      "Epoch 809/1000, Loss: 0.0521\n",
      "Epoch 810/1000, Loss: 0.0669\n",
      "Epoch 811/1000, Loss: 0.0157\n",
      "Epoch 812/1000, Loss: 0.0331\n",
      "Epoch 813/1000, Loss: 0.0781\n",
      "Epoch 814/1000, Loss: 0.0556\n",
      "Epoch 815/1000, Loss: 0.0432\n",
      "Epoch 816/1000, Loss: 0.0380\n",
      "Epoch 817/1000, Loss: 0.0444\n",
      "Epoch 818/1000, Loss: 0.0332\n",
      "Epoch 819/1000, Loss: 0.0503\n",
      "Epoch 820/1000, Loss: 0.0382\n",
      "Epoch 821/1000, Loss: 0.0603\n",
      "Epoch 822/1000, Loss: 0.0528\n",
      "Epoch 823/1000, Loss: 0.0376\n",
      "Epoch 824/1000, Loss: 0.0361\n",
      "Epoch 825/1000, Loss: 0.0298\n",
      "Epoch 826/1000, Loss: 0.0406\n",
      "Epoch 827/1000, Loss: 0.0514\n",
      "Epoch 828/1000, Loss: 0.0538\n",
      "Epoch 829/1000, Loss: 0.0580\n",
      "Epoch 830/1000, Loss: 0.0362\n",
      "Epoch 831/1000, Loss: 0.0326\n",
      "Epoch 832/1000, Loss: 0.0468\n",
      "Epoch 833/1000, Loss: 0.0422\n",
      "Epoch 834/1000, Loss: 0.0487\n",
      "Epoch 835/1000, Loss: 0.0652\n",
      "Epoch 836/1000, Loss: 0.0489\n",
      "Epoch 837/1000, Loss: 0.0574\n",
      "Epoch 838/1000, Loss: 0.0840\n",
      "Epoch 839/1000, Loss: 0.0493\n",
      "Epoch 840/1000, Loss: 0.0349\n",
      "Epoch 841/1000, Loss: 0.0465\n",
      "Epoch 842/1000, Loss: 0.0502\n",
      "Epoch 843/1000, Loss: 0.0462\n",
      "Epoch 844/1000, Loss: 0.0512\n",
      "Epoch 845/1000, Loss: 0.0454\n",
      "Epoch 846/1000, Loss: 0.0624\n",
      "Epoch 847/1000, Loss: 0.0324\n",
      "Epoch 848/1000, Loss: 0.0540\n",
      "Epoch 849/1000, Loss: 0.0359\n",
      "Epoch 850/1000, Loss: 0.0264\n",
      "Epoch 851/1000, Loss: 0.0534\n",
      "Epoch 852/1000, Loss: 0.0337\n",
      "Epoch 853/1000, Loss: 0.0440\n",
      "Epoch 854/1000, Loss: 0.0428\n",
      "Epoch 855/1000, Loss: 0.0434\n",
      "Epoch 856/1000, Loss: 0.0459\n",
      "Epoch 857/1000, Loss: 0.0484\n",
      "Epoch 858/1000, Loss: 0.0582\n",
      "Epoch 859/1000, Loss: 0.0644\n",
      "Epoch 860/1000, Loss: 0.0459\n",
      "Epoch 861/1000, Loss: 0.0538\n",
      "Epoch 862/1000, Loss: 0.0340\n",
      "Epoch 863/1000, Loss: 0.0538\n",
      "Epoch 864/1000, Loss: 0.0310\n",
      "Epoch 865/1000, Loss: 0.0539\n",
      "Epoch 866/1000, Loss: 0.0324\n",
      "Epoch 867/1000, Loss: 0.0474\n",
      "Epoch 868/1000, Loss: 0.0465\n",
      "Epoch 869/1000, Loss: 0.0475\n",
      "Epoch 870/1000, Loss: 0.0638\n",
      "Epoch 871/1000, Loss: 0.0539\n",
      "Epoch 872/1000, Loss: 0.0319\n",
      "Epoch 873/1000, Loss: 0.0287\n",
      "Epoch 874/1000, Loss: 0.0326\n",
      "Epoch 875/1000, Loss: 0.0427\n",
      "Epoch 876/1000, Loss: 0.0278\n",
      "Epoch 877/1000, Loss: 0.0522\n",
      "Epoch 878/1000, Loss: 0.0397\n",
      "Epoch 879/1000, Loss: 0.0215\n",
      "Epoch 880/1000, Loss: 0.0604\n",
      "Epoch 881/1000, Loss: 0.0567\n",
      "Epoch 882/1000, Loss: 0.0388\n",
      "Epoch 883/1000, Loss: 0.0277\n",
      "Epoch 884/1000, Loss: 0.0330\n",
      "Epoch 885/1000, Loss: 0.0335\n",
      "Epoch 886/1000, Loss: 0.0215\n",
      "Epoch 887/1000, Loss: 0.0506\n",
      "Epoch 888/1000, Loss: 0.0395\n",
      "Epoch 889/1000, Loss: 0.0483\n",
      "Epoch 890/1000, Loss: 0.0334\n",
      "Epoch 891/1000, Loss: 0.0713\n",
      "Epoch 892/1000, Loss: 0.0440\n",
      "Epoch 893/1000, Loss: 0.0437\n",
      "Epoch 894/1000, Loss: 0.0335\n",
      "Epoch 895/1000, Loss: 0.0401\n",
      "Epoch 896/1000, Loss: 0.0367\n",
      "Epoch 897/1000, Loss: 0.0468\n",
      "Epoch 898/1000, Loss: 0.0357\n",
      "Epoch 899/1000, Loss: 0.0492\n",
      "Epoch 900/1000, Loss: 0.0702\n",
      "Epoch 901/1000, Loss: 0.0277\n",
      "Epoch 902/1000, Loss: 0.0434\n",
      "Epoch 903/1000, Loss: 0.0242\n",
      "Epoch 904/1000, Loss: 0.0245\n",
      "Epoch 905/1000, Loss: 0.0368\n",
      "Epoch 906/1000, Loss: 0.0503\n",
      "Epoch 907/1000, Loss: 0.0480\n",
      "Epoch 908/1000, Loss: 0.0266\n",
      "Epoch 909/1000, Loss: 0.0401\n",
      "Epoch 910/1000, Loss: 0.0571\n",
      "Epoch 911/1000, Loss: 0.0386\n",
      "Epoch 912/1000, Loss: 0.0585\n",
      "Epoch 913/1000, Loss: 0.0170\n",
      "Epoch 914/1000, Loss: 0.0583\n",
      "Epoch 915/1000, Loss: 0.0386\n",
      "Epoch 916/1000, Loss: 0.0440\n",
      "Epoch 917/1000, Loss: 0.0309\n",
      "Epoch 918/1000, Loss: 0.0397\n",
      "Epoch 919/1000, Loss: 0.0488\n",
      "Epoch 920/1000, Loss: 0.0316\n",
      "Epoch 921/1000, Loss: 0.0364\n",
      "Epoch 922/1000, Loss: 0.0378\n",
      "Epoch 923/1000, Loss: 0.0314\n",
      "Epoch 924/1000, Loss: 0.0366\n",
      "Epoch 925/1000, Loss: 0.0365\n",
      "Epoch 926/1000, Loss: 0.0256\n",
      "Epoch 927/1000, Loss: 0.0608\n",
      "Epoch 928/1000, Loss: 0.0340\n",
      "Epoch 929/1000, Loss: 0.0520\n",
      "Epoch 930/1000, Loss: 0.0380\n",
      "Epoch 931/1000, Loss: 0.0313\n",
      "Epoch 932/1000, Loss: 0.0432\n",
      "Epoch 933/1000, Loss: 0.0500\n",
      "Epoch 934/1000, Loss: 0.0319\n",
      "Epoch 935/1000, Loss: 0.0339\n",
      "Epoch 936/1000, Loss: 0.0407\n",
      "Epoch 937/1000, Loss: 0.0358\n",
      "Epoch 938/1000, Loss: 0.0526\n",
      "Epoch 939/1000, Loss: 0.0260\n",
      "Epoch 940/1000, Loss: 0.0150\n",
      "Epoch 941/1000, Loss: 0.0377\n",
      "Epoch 942/1000, Loss: 0.0427\n",
      "Epoch 943/1000, Loss: 0.0197\n",
      "Epoch 944/1000, Loss: 0.0468\n",
      "Epoch 945/1000, Loss: 0.0403\n",
      "Epoch 946/1000, Loss: 0.0574\n",
      "Epoch 947/1000, Loss: 0.0360\n",
      "Epoch 948/1000, Loss: 0.0480\n",
      "Epoch 949/1000, Loss: 0.0760\n",
      "Epoch 950/1000, Loss: 0.0353\n",
      "Epoch 951/1000, Loss: 0.0714\n",
      "Epoch 952/1000, Loss: 0.0280\n",
      "Epoch 953/1000, Loss: 0.0454\n",
      "Epoch 954/1000, Loss: 0.0500\n",
      "Epoch 955/1000, Loss: 0.0270\n",
      "Epoch 956/1000, Loss: 0.0419\n",
      "Epoch 957/1000, Loss: 0.0302\n",
      "Epoch 958/1000, Loss: 0.0419\n",
      "Epoch 959/1000, Loss: 0.0304\n",
      "Epoch 960/1000, Loss: 0.0330\n",
      "Epoch 961/1000, Loss: 0.0621\n",
      "Epoch 962/1000, Loss: 0.0457\n",
      "Epoch 963/1000, Loss: 0.0291\n",
      "Epoch 964/1000, Loss: 0.0576\n",
      "Epoch 965/1000, Loss: 0.0330\n",
      "Epoch 966/1000, Loss: 0.0371\n",
      "Epoch 967/1000, Loss: 0.0325\n",
      "Epoch 968/1000, Loss: 0.0393\n",
      "Epoch 969/1000, Loss: 0.0418\n",
      "Epoch 970/1000, Loss: 0.0285\n",
      "Epoch 971/1000, Loss: 0.0410\n",
      "Epoch 972/1000, Loss: 0.0587\n",
      "Epoch 973/1000, Loss: 0.0259\n",
      "Epoch 974/1000, Loss: 0.0346\n",
      "Epoch 975/1000, Loss: 0.0320\n",
      "Epoch 976/1000, Loss: 0.0241\n",
      "Epoch 977/1000, Loss: 0.0664\n",
      "Epoch 978/1000, Loss: 0.0183\n",
      "Epoch 979/1000, Loss: 0.0326\n",
      "Epoch 980/1000, Loss: 0.0350\n",
      "Epoch 981/1000, Loss: 0.0308\n",
      "Epoch 982/1000, Loss: 0.0332\n",
      "Epoch 983/1000, Loss: 0.0401\n",
      "Epoch 984/1000, Loss: 0.0473\n",
      "Epoch 985/1000, Loss: 0.0322\n",
      "Epoch 986/1000, Loss: 0.0429\n",
      "Epoch 987/1000, Loss: 0.0468\n",
      "Epoch 988/1000, Loss: 0.0469\n",
      "Epoch 989/1000, Loss: 0.0286\n",
      "Epoch 990/1000, Loss: 0.0503\n",
      "Epoch 991/1000, Loss: 0.0162\n",
      "Epoch 992/1000, Loss: 0.0201\n",
      "Epoch 993/1000, Loss: 0.0376\n",
      "Epoch 994/1000, Loss: 0.0378\n",
      "Epoch 995/1000, Loss: 0.0732\n",
      "Epoch 996/1000, Loss: 0.0595\n",
      "Epoch 997/1000, Loss: 0.0496\n",
      "Epoch 998/1000, Loss: 0.0622\n",
      "Epoch 999/1000, Loss: 0.0619\n",
      "Epoch 1000/1000, Loss: 0.0340\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    DenseLayer(784, 128),\n",
    "    ReLU(),\n",
    "    BatchNormalization(128),\n",
    "    Dropout(0.5),\n",
    "    DenseLayer(128, 64),\n",
    "    ReLU(),\n",
    "    BatchNormalization(64),\n",
    "    Dropout(0.5),\n",
    "    DenseLayer(64, 32),\n",
    "    ReLU(),\n",
    "    BatchNormalization(32),\n",
    "    Dropout(0.5),\n",
    "    DenseLayer(32, 10),\n",
    "    Softmax()\n",
    "]\n",
    "\n",
    "# Initialize Adam optimizer\n",
    "adam = Adam(learning_rate=0.001)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "# Prepare training data\n",
    "X_train, y_train = next(iter(train_loader))\n",
    "X_train = X_train.numpy().reshape(-1, 28*28)  # Flatten the images\n",
    "y_train = y_train.numpy()\n",
    "\n",
    "\n",
    "X_test, y_test = next(iter(test_loader))\n",
    "X_test = X_test.numpy().reshape(-1, 28 * 28)  # Flatten the images\n",
    "y_test = y_test.numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Create and train the network\n",
    "network = FeedForwardNetwork(layers=layers, optimizer=adam, epochs=1000, batch_size=64)\n",
    "network.train(X_train, y_train)\n",
    "\n",
    "network.test(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
